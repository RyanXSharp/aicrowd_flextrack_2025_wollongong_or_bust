{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fb0a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 250)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9d8b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/sites_ABC.csv')\n",
    "df.columns = ['site','timestamp','temp','irr','power','demand_response','demand_response_capacity']\n",
    "\n",
    "non_feature_cols = ['site','timestamp','demand_response','demand_response_capacity','date','busday','time','minute']\n",
    "base_features = ['temp','irr','power']\n",
    "target_cls = 'demand_response'\n",
    "target_reg = 'demand_response_capacity'\n",
    "working_hours = [10,11,12,13,14,15,16,17]\n",
    "relevant_hours = [8,9,10,11,12,13,14,15,16,17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959a65f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df['busday'] = np.is_busday(df['timestamp'].to_numpy().astype(\"datetime64[D]\")).astype(int)\n",
    "df['date'] = pd.to_datetime(df['timestamp'].dt.date)\n",
    "df['day_of_week'] = df['date'].dt.weekday\n",
    "df['week'] = df['timestamp'].dt.isocalendar().week.astype(int)\n",
    "df['hour'] = df['timestamp'].dt.hour\n",
    "df['time'] = df['timestamp'].dt.time\n",
    "df['quarter_hour'] = df['timestamp'].dt.minute//15\n",
    "df['month'] = df['timestamp'].dt.month\n",
    "df['season'] = 0\n",
    "df.loc[df['month'].isin([12,1,2]), 'season'] = 0\n",
    "df.loc[df['month'].isin([3,4,5]), 'season'] = 1\n",
    "df.loc[df['month'].isin([6,7,8]), 'season'] = 2\n",
    "df.loc[df['month'].isin([9,10,11]), 'season'] = 3\n",
    "df['working_hours'] = 0\n",
    "df.loc[df['hour'].isin(working_hours), 'working_hours'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a32ec55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_prediction(group):\n",
    "    dr_nonzero = group['li_flag'] != 0\n",
    "    nonzero_indices = group.index[dr_nonzero]\n",
    "\n",
    "    if nonzero_indices.empty:\n",
    "        group['li_feature'] = np.nan\n",
    "        return group\n",
    "\n",
    "    start_idx = nonzero_indices[0]\n",
    "    end_idx = nonzero_indices[-1]\n",
    "\n",
    "    before_idx = start_idx - 1\n",
    "    after_idx = end_idx + 1\n",
    "\n",
    "    if before_idx < group.index[0] or after_idx > group.index[-1]:\n",
    "        group['li_feature'] = np.nan\n",
    "        return group\n",
    "\n",
    "    start_power = group.loc[before_idx, 'power']\n",
    "    end_power = group.loc[after_idx, 'power']\n",
    "\n",
    "    num_points = end_idx - start_idx + 1\n",
    "    interpolated = np.linspace(start_power, end_power, num_points)\n",
    "\n",
    "    group['li_feature'] = np.nan\n",
    "    group.loc[start_idx:end_idx, 'li_feature'] = interpolated\n",
    "\n",
    "    return group\n",
    "\n",
    "def fill_between_activity(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    def fill_day(g):\n",
    "        # find first and last hour where pow_above_base == 1\n",
    "        if g['pow_above_base'].sum() == 0:\n",
    "            return g  # no active hours, do nothing\n",
    "        first_on = g.loc[g['pow_above_base'] == 1, 'time'].min()\n",
    "        last_on  = g.loc[g['pow_above_base'] == 1, 'time'].max()\n",
    "\n",
    "        # fill all hours between those two with 1\n",
    "        g.loc[(g['time'] >= first_on) & (g['time'] <= last_on), 'pow_above_base'] = 1\n",
    "        return g\n",
    "\n",
    "    df = df.groupby(['site', 'date'], group_keys=False).apply(fill_day)\n",
    "    return df\n",
    "\n",
    "def feature_engineering(og_df):\n",
    "    df = og_df.copy()\n",
    "    \n",
    "    baseline_partition_columns = ['site', 'date']\n",
    "    baseline_hours = [5,6,7]\n",
    "    baseline_powers = df[df['hour'].isin(baseline_hours)].groupby(baseline_partition_columns)['power'].median().to_dict()\n",
    "    df['baseline_pow'] = (\n",
    "        df[baseline_partition_columns]\n",
    "        .apply(tuple, axis=1)\n",
    "        .map(baseline_powers)\n",
    "        * 1.2\n",
    "    )\n",
    "    df['pow_above_base'] = (df['power'] > df['baseline_pow']).astype(int)\n",
    "    df = fill_between_activity(df)\n",
    "    df['site_on'] = df.groupby(['site','date'])['pow_above_base'].cumsum()\n",
    "    df.loc[df['pow_above_base'] == 0, 'site_on'] = 0\n",
    "    \n",
    "    import datetime as dt\n",
    "    df['li_flag'] = df['time'].apply(lambda t: int(dt.time(10,0) <= t <= dt.time(18,0)))\n",
    "    df = df.groupby(['site', 'date'], group_keys=False).apply(interpolate_prediction)\n",
    "\n",
    "    # df = df[df['working_hours'] == 1].reset_index(drop=True).copy()\n",
    "    for var in ['temp', 'irr']:\n",
    "        # per-site, month, time correlation of power with var\n",
    "        corr_map = df.groupby(['site','month','day_of_week','time']).apply(\n",
    "            lambda g: g['power'].corr(g[var])\n",
    "        ).rename(f'{var}_power_corr_mt')\n",
    "\n",
    "        df = df.merge(corr_map, on=['site','month','day_of_week','time'], how='left')\n",
    "\n",
    "        corr_map = df.groupby(['site','season','day_of_week','time']).apply(\n",
    "            lambda g: g['power'].corr(g[var])\n",
    "        ).rename(f'{var}_power_corr_st')\n",
    "\n",
    "        df = df.merge(corr_map, on=['site','season','day_of_week','time'], how='left')\n",
    "\n",
    "        # per-site, month, time correlation of power with var POW ABOVE BASE ONLY\n",
    "        corr_map = df.loc[df['pow_above_base'] == 1].groupby(['site','month','day_of_week','time']).apply(\n",
    "            lambda g: g['power'].corr(g[var])\n",
    "        ).rename(f'{var}_power_corr_mt_pab')\n",
    "\n",
    "        df = df.merge(corr_map, on=['site','month','day_of_week','time'], how='left')\n",
    "\n",
    "        corr_map = df.loc[df['pow_above_base'] == 1].groupby(['site','season','day_of_week','time']).apply(\n",
    "            lambda g: g['power'].corr(g[var])\n",
    "        ).rename(f'{var}_power_corr_st_pab')\n",
    "\n",
    "        df = df.merge(corr_map, on=['site','season','day_of_week','time'], how='left')\n",
    "\n",
    "        # median of var per site, month, time\n",
    "        med_map = df.groupby(['site','month','time'])[var].median().rename(f'{var}_median_mt')\n",
    "        df = df.merge(med_map, on=['site','month','time'], how='left')\n",
    "\n",
    "        med_map = df.groupby(['site','season','time'])[var].median().rename(f'{var}_median_st')\n",
    "        df = df.merge(med_map, on=['site','season','time'], how='left')\n",
    "\n",
    "        # interaction term\n",
    "        df[f'{var}_corr_dev_mt'] = df[f'{var}_power_corr_mt'] * (df[var] - df[f'{var}_median_mt'])\n",
    "        df[f'{var}_corr_dev_st'] = df[f'{var}_power_corr_st'] * (df[var] - df[f'{var}_median_st'])\n",
    "\n",
    "        # interaction term pab\n",
    "        df[f'{var}_corr_dev_mt_pab'] = df[f'{var}_power_corr_mt_pab'] * (df[var] - df[f'{var}_median_mt'])\n",
    "        df[f'{var}_corr_dev_st_pab'] = df[f'{var}_power_corr_st_pab'] * (df[var] - df[f'{var}_median_st'])\n",
    "\n",
    "    df.fillna(0, inplace=True)\n",
    "\n",
    "    for diff in [1,2,4,12]:\n",
    "        df[f\"temp_diff{diff if diff > 1 else ''}\"] = df.groupby(['site'])['temp'].diff(diff).fillna(0)\n",
    "        df[f\"irr_diff{diff if diff > 1 else ''}\"] = df.groupby(['site'])['irr'].diff(diff).fillna(0)\n",
    "\n",
    "    features = [\n",
    "        'temp', 'irr', \n",
    "        'temp_diff', 'irr_diff',\n",
    "        'temp_diff2', 'irr_diff2',\n",
    "        'temp_diff4', 'irr_diff4',\n",
    "        'temp_diff12', 'irr_diff12'\n",
    "    ]\n",
    "    group_levels = [['site', 'time'], ['site']]\n",
    "    suffixes = {\n",
    "        ('site', 'time'): ('lag', 'peek'),\n",
    "        ('site',): ('shift', 'pull')\n",
    "    }\n",
    "    shifts = [1, 2, 3, 4, 8, 12]\n",
    "\n",
    "    for feature in features:\n",
    "        for group in group_levels:\n",
    "            suffix_pair = suffixes[tuple(group)]\n",
    "            for direction, suffix in zip([1, -1], suffix_pair):\n",
    "                for s in shifts:\n",
    "                    col_name = f\"{feature}_{suffix}{s if s > 1 else ''}\"\n",
    "                    df[col_name] = df.groupby(group)[feature].transform(lambda x, shift=s*direction: x.shift(shift))\n",
    "\n",
    "    df['usage_lag'] = df.groupby(['site','time'])['power'].transform(lambda x: x.shift(1))\n",
    "    df['usage_lag2'] = df.groupby(['site','time'])['power'].transform(lambda x: x.shift(2))\n",
    "    df['usage_lag_dow'] = df.groupby(['site','day_of_week','time'])['power'].transform(lambda x: x.shift(1))\n",
    "    df['usage_lag_dow2'] = df.groupby(['site','day_of_week','time'])['power'].transform(lambda x: x.shift(2))\n",
    "\n",
    "    df['usage_peek'] = df.groupby(['site','time'])['power'].transform(lambda x: x.shift(-1))\n",
    "    df['usage_peek2'] = df.groupby(['site','time'])['power'].transform(lambda x: x.shift(-2))\n",
    "    df['usage_peek_dow'] = df.groupby(['site','day_of_week','time'])['power'].transform(lambda x: x.shift(-1))\n",
    "    df['usage_peek_dow2'] = df.groupby(['site','day_of_week','time'])['power'].transform(lambda x: x.shift(-2))\n",
    "\n",
    "    df['mean_usage_mdt'] = df.groupby(['site','month','day_of_week','time'])['power'].transform(lambda x: x.mean())\n",
    "    df['mean_usage_sdt'] = df.groupby(['site','season','day_of_week','time'])['power'].transform(lambda x: x.mean())\n",
    "\n",
    "    df['mean_usage_mdt_corr_dev_mt'] = (df['mean_usage_mdt'] + df['temp_corr_dev_mt']*1.5 + df['irr_corr_dev_mt'] * 0.05)\n",
    "    df['mean_usage_sdt_corr_dev_st'] = (df['mean_usage_sdt'] + df['temp_corr_dev_st']*1.5 + df['irr_corr_dev_st'] * 0.05)\n",
    "\n",
    "    df['mean_usage_mdt_corr_dev_mt_pab'] = (df['mean_usage_mdt'] + df['temp_corr_dev_mt_pab']*1.5 + df['irr_corr_dev_mt_pab'] * 0.05)\n",
    "    df['mean_usage_sdt_corr_dev_st_pab'] = (df['mean_usage_sdt'] + df['temp_corr_dev_st_pab']*1.5 + df['irr_corr_dev_st_pab'] * 0.05)\n",
    "\n",
    "    df['pow_monthly_max'] = df.groupby(['site','month'])['power'].transform('max')\n",
    "\n",
    "    return df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827903a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['day_response']     = 0 # -1,0,1,2\n",
    "\n",
    "positive_flag_dates = df.groupby([\"site\", \"date\"]).filter(\n",
    "    lambda df: (df[\"demand_response\"] == 1).any() and not (df[\"demand_response\"] == -1).any()\n",
    ")[[\"site\", \"date\"]].drop_duplicates()\n",
    "\n",
    "negative_flag_dates = df.groupby([\"site\", \"date\"]).filter(\n",
    "    lambda df: (df[\"demand_response\"] == -1).any() and not (df[\"demand_response\"] == 1).any()\n",
    ")[[\"site\", \"date\"]].drop_duplicates()\n",
    "\n",
    "pos_and_neg_flag_dates = df.groupby([\"site\", \"date\"]).filter(\n",
    "    lambda df: (df[\"demand_response\"] == 1).any() and (df[\"demand_response\"] == -1).any()\n",
    ")[[\"site\", \"date\"]].drop_duplicates()\n",
    "\n",
    "# Create a tuple column for easier comparison\n",
    "df[\"site_date\"] = list(zip(df[\"site\"], df[\"date\"]))\n",
    "\n",
    "# Create sets of tuples for comparison\n",
    "neg_set = set(zip(negative_flag_dates[\"site\"], negative_flag_dates[\"date\"]))\n",
    "pos_set = set(zip(positive_flag_dates[\"site\"], positive_flag_dates[\"date\"]))\n",
    "both_set = set(zip(pos_and_neg_flag_dates[\"site\"], pos_and_neg_flag_dates[\"date\"]))\n",
    "\n",
    "# Assign values\n",
    "df.loc[df[\"site_date\"].isin(neg_set), \"day_response\"] = -1\n",
    "df.loc[df[\"site_date\"].isin(pos_set), \"day_response\"] = 1\n",
    "df.loc[df[\"site_date\"].isin(both_set), \"day_response\"] = 2\n",
    "\n",
    "# Optionally drop the helper column\n",
    "df.drop(columns=\"site_date\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a699ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wh = feature_engineering(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e5e037",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_response = df_wh[(df_wh['day_response'] == 0) & (df_wh['working_hours'] == 1)].copy()\n",
    "print(len(df_no_response)//(10*4))\n",
    "df_no_response.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b46f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_response = df_wh[(df_wh['day_response'] != 0) & (df_wh['working_hours'] == 1)].copy()\n",
    "print(len(df_response)//(10*4))\n",
    "df_response.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d92594",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y, y_hat, return_median=False):\n",
    "  if not return_median:\n",
    "    return np.sqrt(np.mean((y - y_hat) ** 2))\n",
    "  else:\n",
    "    return np.sqrt(np.median((y - y_hat) ** 2))\n",
    "\n",
    "def r_squared(y, y_hat):\n",
    "  ssr = np.sum((y - y_hat) ** 2)\n",
    "  sst = np.sum((y - np.mean(y)) ** 2)\n",
    "  return 1 - (ssr / sst)\n",
    "\n",
    "def mape(y, y_hat, return_median=False):\n",
    "  if not return_median:\n",
    "    return np.mean(np.abs((y - y_hat) / y))*100\n",
    "  else:\n",
    "    return np.median(np.abs((y - y_hat) / y))*100\n",
    "\n",
    "def mpe(y, y_hat, return_median=False): # Negative means over-prediction\n",
    "  if not return_median:\n",
    "    return np.mean((y - y_hat) / y)*100\n",
    "  else:\n",
    "    return np.median((y - y_hat) / y)*100\n",
    "\n",
    "def lw_mape(y, y_hat, add_epsilon=True):\n",
    "  if add_epsilon: # Avoids division by zero (if needed)\n",
    "    epsilon = 1e-8\n",
    "    y += epsilon\n",
    "  w = np.abs(y) / np.abs(y).sum()\n",
    "  return np.sum(np.abs((y - y_hat) / y) * w)*100\n",
    "\n",
    "def evaluate_abc_partitioned_site(df, pred_col='capacity_prediction'):\n",
    "  res = []\n",
    "  for site in ['A', 'B', 'C']:\n",
    "        ixs = (df['site']==f'site{site}')&(df['demand_response']!=0)\n",
    "        res.append({\n",
    "        \"site\":site,\n",
    "        \"RMSE\":rmse(df.loc[ixs, 'demand_response_capacity'], df.loc[ixs, pred_col]),\n",
    "        \"MAE\":mean_absolute_error(df.loc[ixs, 'demand_response_capacity'], df.loc[ixs, pred_col]),\n",
    "        \"W_MAPE (%)\":lw_mape(df.loc[ixs, 'demand_response_capacity'], df.loc[ixs, pred_col]),\n",
    "        \"Num_Days\":len(df.loc[ixs, 'date'].drop_duplicates()),\n",
    "        })\n",
    "  return pd.DataFrame(res).set_index('site')\n",
    "\n",
    "def evaluate_abc_partitioned_site_dayresponse(df, pred_col='capacity_prediction'):\n",
    "  res = []\n",
    "  for site in ['A', 'B', 'C']:\n",
    "    for day_response in [-1,1,2]:\n",
    "        ixs = (df['site']==f'site{site}')&(df['day_response']==day_response)&(df['demand_response']!=0)\n",
    "        res.append({\n",
    "        \"site\":site,\n",
    "        \"day_response\":day_response,\n",
    "        \"RMSE\":rmse(df.loc[ixs, 'demand_response_capacity'], df.loc[ixs, pred_col]),\n",
    "        \"MAE\":mean_absolute_error(df.loc[ixs, 'demand_response_capacity'], df.loc[ixs, pred_col]),\n",
    "        \"W_MAPE (%)\":lw_mape(df.loc[ixs, 'demand_response_capacity'], df.loc[ixs, pred_col]),\n",
    "        \"Num_Days\":len(df.loc[ixs, 'date'].drop_duplicates()),\n",
    "        })\n",
    "  return pd.DataFrame(res).set_index(['site', 'day_response'])\n",
    "\n",
    "def evaluate_abc_partitioned_site_response(df, pred_col='capacity_prediction'):\n",
    "  res = []\n",
    "  for site in ['A', 'B', 'C']:\n",
    "    for demand_response in [-1,1]:\n",
    "        ixs = (df['site']==f'site{site}')&(df['demand_response']==demand_response)&(df['demand_response']!=0)\n",
    "        res.append({\n",
    "        \"site\":site,\n",
    "        \"demand_response\":demand_response,\n",
    "        \"RMSE\":rmse(df.loc[ixs, 'demand_response_capacity'], df.loc[ixs, pred_col]),\n",
    "        \"MAE\":mean_absolute_error(df.loc[ixs, 'demand_response_capacity'], df.loc[ixs, pred_col]),\n",
    "        \"W_MAPE (%)\":lw_mape(df.loc[ixs, 'demand_response_capacity'], df.loc[ixs, pred_col]),\n",
    "        \"Num_Days\":len(df.loc[ixs, 'date'].drop_duplicates()),\n",
    "        })\n",
    "        \n",
    "  return pd.DataFrame(res).set_index(['site', 'demand_response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b442b69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = ['day_of_week', 'month', 'li_feature', 'temp_lag', 'temp_lag3', 'temp_lag4', 'temp_peek', 'irr_lag', 'irr_lag2', 'irr_lag3', 'irr_lag4', 'irr_peek2', 'irr_peek3', 'irr_peek4', 'temp_shift4', 'irr_shift', 'irr_shift2', 'irr_shift3', 'irr_shift4', 'irr_pull', 'irr_pull2', 'irr_pull4', 'usage_lag_dow2', 'usage_lag_dow', 'hour', 'irr_pull3', 'temp_median_mt', 'irr_median_st', 'week', 'temp_pull3', 'irr_power_corr_st', 'irr_peek', 'baseline_pow', 'quarter_hour', 'temp_pull2', 'temp_lag2'] \n",
    "feats2 = ['irr_lag', 'irr_peek3', 'temp_lag', 'temp_peek2', 'season', 'month', 'day_of_week', 'hour', 'temp', 'irr', 'mean_usage_sdt_corr_dev_st', 'usage_lag', 'usage_lag_dow', 'usage_peek', 'usage_peek_dow', 'temp_shift3', 'temp_lag3', 'li_feature', 'baseline_pow', 'temp_pull3', 'temp_shift12', 'irr_diff4_shift3'] \n",
    "feats3 = ['day_of_week', 'hour', 'month', 'mean_usage_sdt_corr_dev_st', 'li_feature', 'temp_lag', 'temp_shift4', 'week', 'irr_power_corr_st_pab', 'mean_usage_sdt', 'baseline_pow', 'usage_peek2', 'irr_median_mt', 'irr_power_corr_st', 'usage_lag', 'temp_shift', 'temp_power_corr_st', 'irr_shift4', 'temp_lag3', 'irr_pull2', ] \n",
    "feats4 = ['day_of_week', 'hour', 'month', 'li_feature', 'temp_shift4', 'temp_lag', 'irr_diff2_peek2', 'week', 'irr_diff12_pull', 'temp_median_st', 'mean_usage_sdt_corr_dev_st_pab', 'temp_diff12_shift4', 'irr_diff4_pull3', 'usage_lag_dow', 'temp_lag2'] \n",
    "feats5 = ['day_of_week', 'hour', 'month', 'li_feature', 'temp_shift12', 'week', 'temp_lag', 'mean_usage_sdt_corr_dev_st_pab', 'irr_diff4_shift4', 'baseline_pow', 'irr_diff_peek12', 'irr_diff4_shift2', 'pow_monthly_max', 'irr_diff4_lag8', 'irr_diff12_lag12', 'temp_shift', 'usage_lag_dow2', 'irr_diff2_pull8'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c278314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models and their feature sets\n",
    "models = {\n",
    "    'xgbr': (xgb.XGBRegressor(objective='reg:squarederror',random_state=42,eta=0.05778072586944159,subsample=0.6616312346972526,max_depth=9,colsample_bytree=0.6352137434228233, n_estimators=184), feats),\n",
    "    'xgbr2': (xgb.XGBRegressor(objective='reg:squarederror',random_state=42,eta=0.05513689802291822,subsample=0.5167731642508813,max_depth=9,colsample_bytree=0.7395754768661291, n_estimators=104), feats2),\n",
    "    'xgbr3': (xgb.XGBRegressor(objective='reg:absoluteerror',random_state=42,eta=0.06449597248839985,subsample=0.9767027104796806,max_depth=8,colsample_bytree=0.450212514664335, n_estimators=197), feats3),\n",
    "    'xgbr4': (xgb.XGBRegressor(objective='reg:squarederror',random_state=42,eta=0.0636975789822504,subsample=0.6608736639449571,max_depth=9,colsample_bytree=0.6033560282663871, n_estimators=162), feats4),\n",
    "    'xgbr5': (xgb.XGBRegressor(objective='reg:absoluteerror',random_state=42,eta=0.06449597248839985,subsample=0.9767027104796806,max_depth=8,colsample_bytree=0.450212514664335, n_estimators=300), feats5),\n",
    "}\n",
    "\n",
    "# Initialize prediction columns\n",
    "for model_name in models:\n",
    "    df_response[f'power_prediction_{model_name}'] = 0.0\n",
    "    df_response[f'capacity_prediction_{model_name}'] = 0.0\n",
    "\n",
    "# Train and predict per site\n",
    "for site in ['A', 'B', 'C']:\n",
    "    site_label = f'site{site}'\n",
    "\n",
    "    sub_no_response = df_no_response[df_no_response['site'] == site_label].copy()\n",
    "    sub_response = df_response[df_response['site'] == site_label]      \n",
    "\n",
    "    y = sub_no_response['power']\n",
    "\n",
    "    for model_name, (model, features) in models.items():\n",
    "        X_train = sub_no_response[features]\n",
    "        X_test = sub_response[features]\n",
    "\n",
    "        model.fit(X_train, y)\n",
    "        predictions = model.predict(X_test)\n",
    "\n",
    "        # Store predictions\n",
    "        df_response.loc[df_response['site'] == site_label, f'power_prediction_{model_name}'] = predictions\n",
    "\n",
    "# Zero out predictions where demand_response == 0\n",
    "for model_name in models:\n",
    "    mask = df_response['demand_response'] == 0\n",
    "    df_response.loc[mask, f'power_prediction_{model_name}'] = 0.0\n",
    "# \n",
    "# Compute capacity predictions where demand_response != 0\n",
    "for model_name in models:\n",
    "    mask = df_response['demand_response'] != 0\n",
    "    df_response.loc[mask, f'capacity_prediction_{model_name}'] = (\n",
    "        df_response.loc[mask, 'power'] - df_response.loc[mask, f'power_prediction_{model_name}']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d31cd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_response['full_capacity_prediction'] = df_response['capacity_prediction_xgbr'] * 0.45 + df_response['capacity_prediction_xgbr2'] * 0.3 + df_response['capacity_prediction_xgbr3'] * 0.15 + df_response['capacity_prediction_xgbr4'] * 0.1\n",
    "df_response['full_capacity_prediction'] = df_response['capacity_prediction_xgbr'] * 0.45 + df_response['capacity_prediction_xgbr2'] * 0.3 + df_response['capacity_prediction_xgbr3'] * 0.15 + df_response['capacity_prediction_xgbr4'] * 0.1\n",
    "df_response.loc[df_response['demand_response'] == -1, 'full_capacity_prediction'] = df_response.loc[df_response['demand_response'] == -1]['capacity_prediction_xgbr'] * 0.1 + df_response.loc[df_response['demand_response'] == -1]['capacity_prediction_xgbr2'] * 0.1 + df_response.loc[df_response['demand_response'] == -1]['capacity_prediction_xgbr3'] * 0.1 + df_response.loc[df_response['demand_response'] == -1]['capacity_prediction_xgbr4'] * 0.1 + df_response.loc[df_response['demand_response'] == -1]['capacity_prediction_xgbr5'] * 0.6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c36958",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_cols = [col for col in df_response.columns if 'full_capacity_prediction' in col and 'clipped' not in col]\n",
    "# pred_cols = [col for col in df_response.columns if 'capacity_prediction' in col and 'full' not in col]\n",
    "\n",
    "for pred_col in pred_cols:\n",
    "    print(pred_col)\n",
    "    print(evaluate_abc_partitioned_site(df_response, pred_col=pred_col).round(2), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e40cb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pred_col in pred_cols:\n",
    "    print(pred_col)\n",
    "    print(evaluate_abc_partitioned_site_dayresponse(df_response, pred_col=pred_col).round(2), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c76896a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pred_col in pred_cols:\n",
    "    print(pred_col)\n",
    "    print(evaluate_abc_partitioned_site_response(df_response, pred_col=pred_col).round(2), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178fe94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_site_neg_clips = (-.25, .225)\n",
    "small_site_pos_clips = (0, .4)\n",
    "large_site_neg_clips = (-.2, .125)\n",
    "large_site_pos_clips = (0.0, .3)\n",
    "\n",
    "for site in ['A', 'B', 'C']:\n",
    "    site_label = f'site{site}'\n",
    "    max_pow = df_wh[(df_wh['site'] == site_label) & (df_wh['working_hours'] == 1)]['power'].max()\n",
    "    if max_pow < 100:\n",
    "        # neg clips\n",
    "        df_response.loc[(df_response['site'] == site_label) & (df_response['demand_response'] == -1), 'full_capacity_prediction_clipped'] = np.clip(df_response.loc[(df_response['site'] == site_label) & (df_response['demand_response'] == -1)]['full_capacity_prediction']*1.04, small_site_neg_clips[0]*max_pow, small_site_neg_clips[1]*max_pow)\n",
    "        # pos clips\n",
    "        df_response.loc[(df_response['site'] == site_label) & (df_response['demand_response'] == 1), 'full_capacity_prediction_clipped'] = np.clip(df_response.loc[(df_response['site'] == site_label) & (df_response['demand_response'] == 1)]['full_capacity_prediction']*1.03, small_site_pos_clips[0]*max_pow, small_site_pos_clips[1]*max_pow)\n",
    "        # neg clips\n",
    "        # df_response.loc[(df_response['site'] == site_label) & (df_response['demand_response'] == -1), 'full_capacity_prediction_clipped'] = np.clip(df_response.loc[(df_response['site'] == site_label) & (df_response['demand_response'] == -1)]['full_capacity_prediction'], small_site_neg_clips[0]*max_pow, small_site_neg_clips[1]*max_pow)\n",
    "        # pos clips\n",
    "        # df_response.loc[(df_response['site'] == site_label) & (df_response['demand_response'] == 1), 'full_capacity_prediction_clipped'] = np.clip(df_response.loc[(df_response['site'] == site_label) & (df_response['demand_response'] == 1)]['full_capacity_prediction'], small_site_pos_clips[0]*max_pow, small_site_pos_clips[1]*max_pow)\n",
    "    else:\n",
    "        # neg clips\n",
    "        df_response.loc[(df_response['site'] == site_label) & (df_response['demand_response'] == -1), 'full_capacity_prediction_clipped'] = np.clip(df_response.loc[(df_response['site'] == site_label) & (df_response['demand_response'] == -1)]['full_capacity_prediction']*1.04, large_site_neg_clips[0]*max_pow, large_site_neg_clips[1]*max_pow)\n",
    "        # pos clips\n",
    "        df_response.loc[(df_response['site'] == site_label) & (df_response['demand_response'] == 1), 'full_capacity_prediction_clipped'] = np.clip(df_response.loc[(df_response['site'] == site_label) & (df_response['demand_response'] == 1)]['full_capacity_prediction']*1.03, large_site_pos_clips[0]*max_pow, large_site_pos_clips[1]*max_pow)\n",
    "        # neg clips\n",
    "        # df_response.loc[(df_response['site'] == site_label) & (df_response['demand_response'] == -1), 'full_capacity_prediction_clipped'] = np.clip(df_response.loc[(df_response['site'] == site_label) & (df_response['demand_response'] == -1)]['full_capacity_prediction'], large_site_neg_clips[0]*max_pow, large_site_neg_clips[1]*max_pow)\n",
    "        # pos clips\n",
    "        # df_response.loc[(df_response['site'] == site_label) & (df_response['demand_response'] == 1), 'full_capacity_prediction_clipped'] = np.clip(df_response.loc[(df_response['site'] == site_label) & (df_response['demand_response'] == 1)]['full_capacity_prediction'], large_site_pos_clips[0]*max_pow, large_site_pos_clips[1]*max_pow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655b9845",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_cols = [col for col in df_response.columns if 'full_capacity_prediction' in col]\n",
    "# pred_cols = [col for col in df_response.columns if 'capacity_prediction' in col]\n",
    "\n",
    "for pred_col in pred_cols:\n",
    "    print(pred_col)\n",
    "    print(evaluate_abc_partitioned_site(df_response, pred_col=pred_col).round(2), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2184cab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_capacity_prediction_clipped\n",
    "#        RMSE    MAE  W_MAPE (%)  Num_Days\n",
    "# site                                    \n",
    "# A      1.17   0.81       39.77        28\n",
    "# B      1.46   1.06       31.19        41\n",
    "# C     21.05  12.84       47.21        46 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5506a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "# ===============================\n",
    "# Objective function\n",
    "# ===============================\n",
    "def objective(trial, model_name, df_no_response, df_response, features):\n",
    "    scores = []\n",
    "\n",
    "    for site in ['A', 'B', 'C']:\n",
    "        site_label = f'site{site}'\n",
    "        sub_no_response = df_no_response[df_no_response['site'] == site_label].copy()\n",
    "        sub_response = df_response[df_response['site'] == site_label].copy()\n",
    "\n",
    "        y = sub_no_response['power']\n",
    "\n",
    "        # Choose model + hyperparams\n",
    "        if model_name == \"xgbr\":\n",
    "            params = {\n",
    "                \"objective\": \"reg:absoluteerror\",\n",
    "                \"random_state\": 42,\n",
    "                # \"eta\": trial.suggest_float(\"eta\", 0.04, 0.1),\n",
    "                # \"eta\": trial.suggest_categorical(\"eta\", [0.07263299854682012, 0.06335869128027055]),\n",
    "                \"eta\": 0.06449597248839985,\n",
    "                # \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "                \"subsample\": 0.9767027104796806,\n",
    "                # \"max_depth\": trial.suggest_int(\"max_depth\", 8, 12),\n",
    "                \"max_depth\": 8,\n",
    "                # \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.4, 1.0),\n",
    "                \"colsample_bytree\": 0.450212514664335,\n",
    "                \"n_estimators\": trial.suggest_int(\"n_estimators\", 250, 400)\n",
    "            }\n",
    "            model = xgb.XGBRegressor(**params)\n",
    "\n",
    "        # Train model\n",
    "        X_train = sub_no_response[features]\n",
    "        X_test = sub_response[features]\n",
    "        model.fit(X_train, y)\n",
    "        predictions = model.predict(X_test)\n",
    "\n",
    "        # Zero-out demand_response == 0\n",
    "        sub_response[\"power_pred\"] = predictions\n",
    "        sub_response.loc[sub_response['demand_response'] == 0, \"power_pred\"] = 0.0\n",
    "\n",
    "        # Capacity prediction\n",
    "        sub_response[\"capacity_pred\"] = 0.0\n",
    "        mask = sub_response['demand_response'] != 0\n",
    "        sub_response.loc[mask, \"capacity_pred\"] = (\n",
    "            sub_response.loc[mask, \"power\"] - sub_response.loc[mask, \"power_pred\"]\n",
    "        )\n",
    "\n",
    "        power_mean = pd.concat([\n",
    "            sub_no_response.loc[sub_no_response['power'] != 0, 'power'],\n",
    "            sub_response.loc[sub_response['power'] != 0, 'power']\n",
    "        ]).mean()\n",
    "\n",
    "        mae = mean_absolute_error(\n",
    "            sub_response.loc[mask, \"demand_response_capacity\"],\n",
    "            sub_response.loc[mask, \"capacity_pred\"]\n",
    "        )\n",
    "\n",
    "        n_mae_mean = (mae / power_mean) * 100\n",
    "        scores.append(n_mae_mean)\n",
    "        \n",
    "    return np.mean(scores)\n",
    "\n",
    "# ===============================\n",
    "# Run study\n",
    "# ===============================\n",
    "def run_optuna(model_name, df_no_response, df_response, features, n_trials=50):\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(lambda trial: objective(trial, model_name, df_no_response, df_response, features),\n",
    "                   n_trials=n_trials)\n",
    "    print(f\"Best trial for {model_name}: {study.best_trial.params}\")\n",
    "    return study\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165cb604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'eta': 0.05778072586944159, 'subsample': 0.6616312346972526, 'max_depth': 9, 'colsample_bytree': 0.6352137434228233} 10.321465712910529\n",
    "# {'eta': 0.05513689802291822, 'subsample': 0.5167731642508813, 'colsample_bytree': 0.7395754768661291} 10.751949817155186\n",
    "# {'eta': 0.06449597248839985, 'subsample': 0.9767027104796806, 'max_depth': 8, 'colsample_bytree': 0.450212514664335} 10.488023979638237 || 10.4\n",
    "# {'eta': 0.06335869128027055, 'subsample': 0.8518532235993314, 'max_depth': 9, 'colsample_bytree': 0.7880229765180591} 10.50\n",
    "study_xgb = run_optuna(\"xgbr\", df_no_response, df_response, selected_features, n_trials=50) # 10.188453141953326\n",
    "# study_lgb = run_optuna(\"lgbr\", df_no_response, df_response, selected_features, n_trials=100)\n",
    "# study_cat = run_optuna(\"cat\", df_no_response, df_response, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912ac3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "study_xgb.trials_dataframe().drop_duplicates(subset=[col for col in study_xgb.trials_dataframe().columns if 'params_' in col]).sort_values(by='value').head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d494babb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# study_xgb.trials_dataframe().sort_values(by='value').head(10).reset_index(drop=True) # 5.767547\n",
    "# study_lgb.trials_dataframe().sort_values(by='value').head(10).reset_index(drop=True)\n",
    "# study_cat.trials_dataframe().sort_values(by='value').head(10).reset_index(drop=True).to_csv('cat_optuna.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda3e905",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def feature_selection(df_no_response, df_response, model, starting_features, features, target='demand_response_capacity'):\n",
    "    results = []\n",
    "\n",
    "    # Baseline with starting features\n",
    "    baseline_score = evaluate_features(df_no_response, df_response, starting_features, target=target, model=model)\n",
    "    print(f\"Baseline score: {baseline_score:.4f}\")\n",
    "\n",
    "    for feat in tqdm(features, desc=\"Testing features\", unit=\"feat\"):\n",
    "        if feat not in starting_features:\n",
    "            test_feats = starting_features + [feat]\n",
    "            # print(f\"Testing feature: {feat}\")\n",
    "\n",
    "            score = evaluate_features(df_no_response, df_response, test_feats, target=target, model=model)\n",
    "            improvement = score - baseline_score\n",
    "\n",
    "            results.append({\n",
    "                \"feature\": feat,\n",
    "                \"score\": score,\n",
    "                \"improvement\": improvement\n",
    "            })\n",
    "\n",
    "    results_df = pd.DataFrame(results).sort_values(\"improvement\")\n",
    "    return results_df\n",
    "\n",
    "def feature_ablation(df_no_response, df_response, model, feature_list, dnt_list, target='demand_response_capacity'):\n",
    "    results = []\n",
    "\n",
    "    # Baseline with all features\n",
    "    baseline_score = evaluate_features(df_no_response, df_response, feature_list, target=target, model=model)\n",
    "    print(f\"Baseline score: {baseline_score:.4f}\")\n",
    "\n",
    "    features_to_test = [f for f in feature_list if f not in dnt_list]\n",
    "    # Iterate by removing each feature once\n",
    "    for feat in tqdm(features_to_test, desc=\"Ablating features\", unit=\"feat\"):\n",
    "        reduced_feats = [f for f in feature_list if f != feat]\n",
    "        score = evaluate_features(df_no_response, df_response, reduced_feats, target=target, model=model)\n",
    "        change = score - baseline_score\n",
    "\n",
    "        results.append({\n",
    "            \"feature\": feat,\n",
    "            \"score\": score,\n",
    "            \"change_vs_full\": change\n",
    "        })\n",
    "\n",
    "    results_df = pd.DataFrame(results).sort_values(\"change_vs_full\")\n",
    "    return results_df\n",
    "\n",
    "def evaluate_features(df_no_response, df_response, feature_list, target=\"demand_response_capacity\", model=None):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_no_response : pd.DataFrame\n",
    "        Subset of data where demand_response == 0 (training data).\n",
    "    df_response : pd.DataFrame\n",
    "        Subset of data where demand_response != 0 (prediction/evaluation data).\n",
    "    feature_list : list\n",
    "        Features to use.\n",
    "    target : str\n",
    "        Target column to evaluate against (default: demand_response_capacity).\n",
    "    model : object\n",
    "        Any sklearn-style model with .fit() and .predict().\n",
    "    \"\"\"\n",
    "    if model is None:\n",
    "        raise ValueError(\"You must pass a model instance with .fit() and .predict() methods.\")\n",
    "\n",
    "    df_eval = df_response.copy()\n",
    "    df_eval[\"capacity_pred\"] = 0.0\n",
    "\n",
    "    for site in df_no_response[\"site\"].unique():\n",
    "        site_label = site\n",
    "        sub_no_response = df_no_response[df_no_response[\"site\"] == site_label].copy()\n",
    "        sub_response = df_eval[df_eval[\"site\"] == site_label].copy()\n",
    "\n",
    "        y_train = sub_no_response[\"power\"]\n",
    "        X_train = sub_no_response[feature_list]\n",
    "        X_test = sub_response[feature_list]\n",
    "\n",
    "        # Train model\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict power\n",
    "        preds = model.predict(X_test)\n",
    "\n",
    "        # Zero-out where demand_response == 0\n",
    "        sub_response[\"power_pred\"] = 0.0\n",
    "        mask = sub_response[\"demand_response\"] != 0\n",
    "        sub_response.loc[mask, \"power_pred\"] = preds[mask]\n",
    "\n",
    "        # Compute capacity predictions\n",
    "        sub_response[\"capacity_pred\"] = 0.0\n",
    "        sub_response.loc[mask, \"capacity_pred\"] = (\n",
    "            sub_response.loc[mask, \"power\"] - sub_response.loc[mask, \"power_pred\"]\n",
    "        )\n",
    "\n",
    "        # Update df_eval\n",
    "        df_eval.loc[df_eval[\"site\"] == site_label, \"capacity_pred\"] = sub_response[\"capacity_pred\"].values\n",
    "\n",
    "    nmaes = []\n",
    "    for site in df_no_response[\"site\"].unique():\n",
    "        power_mean = pd.concat([df_no_response.loc[(df_no_response['site'] == site) & (df_no_response['power'] != 0)]['power'], df_eval.loc[(df_eval['site'] == site) & (df_eval['power'] != 0)]['power']]).mean()\n",
    "        mae = mean_absolute_error(df_eval.loc[(df_eval['site'] == site) & (df_eval[\"demand_response\"] != 0), target], df_eval.loc[(df_eval['site'] == site) & (df_eval[\"demand_response\"] != 0), \"capacity_pred\"])\n",
    "        n_mae_mean = (mae / power_mean)\n",
    "        response_dates = len(df_eval['date'].drop_duplicates())\n",
    "        nmaes.append(n_mae_mean * response_dates)\n",
    "\n",
    "    # Evaluate on demand_response != 0 rows\n",
    "    return round(np.mean(nmaes), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca22b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = [col for col in df_wh.columns if col not in non_feature_cols + ['demand_response','demand_response_capacity','power','day_response','li_flag','working_hours'] + [col for col in df_wh.columns if 'residual' in col]]\n",
    "print(len(all_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24beae1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "[col for col in all_features if col in feats and col in feats2 and col in feats3 and col in feats4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0262226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_model = xgb.XGBRegressor(objective='reg:squarederror',random_state=42,eta=0.05778072586944159,subsample=0.6616312346972526,max_depth=9,colsample_bytree=0.6352137434228233, n_estimators=184)\n",
    "# test_model = xgb.XGBRegressor(objective='reg:squarederror',random_state=42,eta=0.05513689802291822,subsample=0.5167731642508813,max_depth=9,colsample_bytree=0.7395754768661291, n_estimators=104)\n",
    "# test_model = xgb.XGBRegressor(objective='reg:absoluteerror',random_state=42,eta=0.06449597248839985,subsample=0.9767027104796806,max_depth=8,colsample_bytree=0.450212514664335, n_estimators=197)\n",
    "# test_model = xgb.XGBRegressor(objective='reg:squarederror',random_state=42,eta=0.0636975789822504,subsample=0.6608736639449571,max_depth=9,colsample_bytree=0.6033560282663871, n_estimators=162)\n",
    "# test_model = xgb.XGBRegressor(objective='reg:absoluteerror',random_state=42,eta=0.06449597248839985,subsample=0.9767027104796806,max_depth=8,colsample_bytree=0.450212514664335, n_estimators=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517694a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_features = ['day_of_week','hour','month','li_feature'] + [col for col in all_features if any(item in col for item in ['temp','irr'])]\n",
    "base_features = ['day_of_week','hour','month','li_feature']\n",
    "print(len(base_features))\n",
    "print(base_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1693cc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = list(set(feats+feats2+feats3+feats4)).copy()\n",
    "# selected_features = feats4.copy()\n",
    "# selected_features = base_features.copy()\n",
    "print(len(selected_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420d5c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['day_of_week', 'hour', 'month', 'li_feature', 'temp_shift12', 'week', 'temp_lag', 'mean_usage_sdt_corr_dev_st_pab', 'irr_diff4_shift4', 'baseline_pow', 'irr_diff_peek12', 'irr_diff4_shift2', 'pow_monthly_max', 'irr_diff4_lag8', 'irr_diff12_lag12', 'temp_shift', 'usage_lag_dow2', 'irr_diff2_pull8'] \n",
    "print(len(selected_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a53048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best scores 10.362\n",
    "baseline_score = evaluate_features(df_no_response, df_response, selected_features, target=target_reg, model=test_model)\n",
    "print(baseline_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735facff",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = 10.7302\n",
    "while True:\n",
    "    feature_results = feature_selection(df_no_response, df_response, test_model, \n",
    "                                        starting_features=selected_features, \n",
    "                                        features=[f for f in all_features if f not in selected_features])\n",
    "                                        # features=[f for f in all_features if f not in selected_features and any(item in f for item in ['temp_','irr_']) and not any(item in f for item in ['_corr','_median'])])\n",
    "    new_ft = feature_results['feature'].iloc[0]\n",
    "    new_score = feature_results['score'].iloc[0]\n",
    "    score = new_score\n",
    "    selected_features = selected_features + [new_ft]\n",
    "    print(new_score, new_ft)\n",
    "    print(selected_features, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856d27ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = 10.5065\n",
    "while True:\n",
    "    feature_results = feature_ablation(df_no_response, df_response, test_model, selected_features, dnt_list=[])\n",
    "    new_ft = feature_results['feature'].iloc[0]\n",
    "    new_score = feature_results['score'].iloc[0]\n",
    "    score = new_score\n",
    "    selected_features = [f for f in selected_features if f != new_ft]\n",
    "    print(new_score, new_ft)\n",
    "    print(selected_features, '\\n')\n",
    "    # else:\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00c8843",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(selected_features))\n",
    "print(selected_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
